# -*- coding: utf-8 -*-
"""Word2Vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xVCpVyTmiA4GGWg2oAq3bzBrsM20NkXR

# **Entrenamiento de modelo con tecnica Word2Vec**
"""

!pip install datasets

"""**Corpus**: Documento extenso que contiene instrucciones del lenguaje"""

from gensim.models import Word2Vec # gensim modelo que se va a entrenar y que entienda de embedings a diferentes palabras para ello se usara Word2Vec
import pandas as pd
import re
from gensim.parsing.preprocessing import strip_punctuation, strip_numeric, strip_short, stem_text
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

"""# Cargar los datos que vamos a utilizar desde un data set de huggingface llamado large_spanish_corpus especificamente se usara el corpus ParaCrawl"""

from datasets import load_dataset
dataset_corpus = load_dataset("large_spanish_corpus", "ParaCrawl")

"""# ver los datos de entrenamientos descargados y separados en el objeto dataset_corpus

Trabajar con corpus con mas completos el anterior es muy pequeno
"""

dataset_corpus

subset = dataset_corpus['train'].select(range(1000000))

subset[0:2]

"""# Librerias para procesar y limpiar la informacion del corpus"""

import nltk
nltk.download('stopwords') # stopwords: son las palabras que no aportan valor al dataset
nltk.download('punkt') # punkt: sirve para tokenizar las oraciones
from nltk.tokenize import word_tokenize

"""# Preprocesamiento del corpus

Cada corpus se limpia de distinta manera
"""

# STOP_WORDS = set(stopwords.words('spanish')) # 1


def clean_text(sentence_batch):
  # extrae el texto de la entrada
  text_list = sentence_batch['text']
  cleaned_text_list = []
  for text in text_list:

    # Alternative to next code
    # filtered_text = [word for word in word_tokens if word not in STOP_WORDS] # 2


    # convierte el texto a minusculas
    text = text.lower()

    # Elimina URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # Elimina las menciones @ y '#' de las redes sociales
    text = re.sub(r'\@\W+|\#\w+', '', text)

    # Elimina los caracteres de puntuacion
    text = strip_punctuation(text)

    # Elimina los numeros
    text = strip_numeric(text)

    # Elimina las palabras cortas
    text = strip_short(text, minsize=2)

    # Elimina las palabras comunes (stop words)
    stop_words = set(stopwords.words('spanish'))
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word not in stop_words]

    cleaned_text_list.append(filtered_text)

  # Devolver el texto limpio
  return {'text': cleaned_text_list}

"""# Funcion de procesamiento de lenguaje al corpus"""

sentences_corpus = subset.map(clean_text, batched=True)

"""Resultado"""

sentences_corpus['text'][:3]

"""# Cargar y uso de modelo de embeddings Word2Vec para pasar palabras a vectores

## Generando el entrenamiento
"""

model = Word2Vec(sentences_corpus['text'], vector_size=100, window=5, min_count=2, workers=6, sg=1) # sentences_corpus['text']:  Lo entrenamos con sentences_corpus, vector_size: definir la dimencion del tamano que vamos a tener

# Podemos guardar el modelo para uso futuro
model.save("word2vec.model")

"""Prueba de la vectorizacion"""

model.wv['rey']

"""### Obtener palabras similares de la vectorizacion anterior"""

model.wv.most_similar(['comida'], topn=5)

# model.wv.most_similar(['ser'], topn=5)

# model.wv.most_similar(['reina'], topn=5)

# model.wv.most_similar(['television'], topn=5)

"""### Separar wv, vectores y palabras"""

word_vectors = model.wv
vectors = word_vectors.vectors
words = word_vectors.index_to_key

vectors

"""# Almacenamiento de embeddings"""

df_vectors = pd.DataFrame(vectors)
df_vectors.to_csv('embeddings.tsv', sep='\t', index=False) # se guarda el csv en embeddings.tsv son separador de espacio \t en lugar de coma y no tendra el indece del vector vectorizado

df_words = pd.DataFrame(words)
df_words.to_csv('labels.tsv', sep='\t', index=False)

"""# Usar https://projector.tensorflow.org/ para convertir en 2 o 3 dimenciones los vectores"""